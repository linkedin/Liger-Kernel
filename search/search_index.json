{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#liger-kernel-efficient-triton-kernels-for-llm-training","title":"Liger Kernel: Efficient Triton Kernels for LLM Training","text":"Stable Nightly Discord Build <p>Liger Kernel is a collection of Triton kernels designed specifically for LLM training. It can effectively increase multi-GPU training throughput by 20% and reduces memory usage by 60%. We have implemented Hugging Face Compatible <code>RMSNorm</code>, <code>RoPE</code>, <code>SwiGLU</code>, <code>CrossEntropy</code>, <code>FusedLinearCrossEntropy</code>, and more to come. The kernel works out of the box with Flash Attention, PyTorch FSDP, and Microsoft DeepSpeed. We welcome contributions from the community to gather the best kernels for LLM training.</p> <p>We've also added optimized Post-Training kernels that deliver up to 80% memory savings for alignment and distillation tasks. We support losses like DPO, CPO, ORPO, SimPO, JSD, and many more. Check out how we optimize the memory.</p>"},{"location":"#supercharge-your-model-with-liger-kernel","title":"Supercharge Your Model with Liger Kernel","text":"<p>With one line of code, Liger Kernel can increase throughput by more than 20% and reduce memory usage by 60%, thereby enabling longer context lengths, larger batch sizes, and massive vocabularies.</p> Speed Up Memory Reduction <p>Note: - Benchmark conditions: LLaMA 3-8B, Batch Size = 8, Data Type = <code>bf16</code>, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 8 A100s. - Hugging Face models start to OOM at a 4K context length, whereas Hugging Face + Liger Kernel scales up to 16K.</p>"},{"location":"#optimize-post-training-with-liger-kernel","title":"Optimize Post Training with Liger Kernel","text":"<p>We provide optimized post training kernels like DPO, ORPO, SimPO, and more which can reduce memory usage by up to 80%. You can easily use them as python modules.</p> <pre><code>from liger_kernel.chunked_loss import LigerFusedLinearDPOLoss\norpo_loss = LigerFusedLinearORPOLoss()\ny = orpo_loss(lm_head.weight, x, target)\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Ease of use: Simply patch your Hugging Face model with one line of code, or compose your own model using our Liger Kernel modules.</li> <li>Time and memory efficient: In the same spirit as Flash-Attn, but for layers like RMSNorm, RoPE, SwiGLU, and CrossEntropy! Increases multi-GPU training throughput by 20% and reduces memory usage by 60% with kernel fusion, in-place replacement, and chunking techniques.</li> <li>Exact: Computation is exact\u2014no approximations! Both forward and backward passes are implemented with rigorous unit tests and undergo convergence testing against training runs without Liger Kernel to ensure accuracy.</li> <li>Lightweight: Liger Kernel has minimal dependencies, requiring only Torch and Triton\u2014no extra libraries needed! Say goodbye to dependency headaches!</li> <li>Multi-GPU supported: Compatible with multi-GPU setups (PyTorch FSDP, DeepSpeed, DDP, etc.).</li> <li>Trainer Framework Integration: Axolotl, LLaMa-Factory, SFTTrainer, Hugging Face Trainer, SWIFT</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the stable version:</p> <pre><code>$ pip install liger-kernel\n</code></pre> <p>To install the nightly version:</p> <pre><code>$ pip install liger-kernel-nightly\n</code></pre> <p>To install from source:</p> <pre><code>git clone https://github.com/linkedin/Liger-Kernel.git\ncd Liger-Kernel\n\n# Install Default Dependencies\n# Setup.py will detect whether you are using AMD or NVIDIA\npip install -e .\n\n# Setup Development Dependencies\npip install -e \".[dev]\"\n</code></pre> <p> Dependencies </p> <p>Optional Dependencies </p> <ul> <li><code>transformers &gt;= 4.x</code>: Required if you plan to use the transformers models patching APIs. The specific model you are working will dictate the minimum version of transformers.</li> </ul> <p>Note</p> <p>Our kernels inherit the full spectrum of hardware compatibility offered by Triton.</p>"},{"location":"#cuda","title":"CUDA","text":"<ul> <li><code>torch &gt;= 2.1.2</code></li> <li><code>triton &gt;= 2.3.0</code></li> </ul>"},{"location":"#rocm","title":"ROCm","text":"<ul> <li><code>torch &gt;= 2.5.0</code> Install according to the instruction in Pytorch official webpage.</li> <li><code>triton &gt;= 3.0.0</code> Install from pypi. (e.g. <code>pip install triton==3.0.0</code>)</li> </ul>"},{"location":"#sponsorship-and-collaboration","title":"Sponsorship and Collaboration","text":"<ul> <li>AMD: Providing AMD GPUs for our AMD CI.</li> <li>Intel: Providing Intel GPUs for our Intel CI.</li> <li>Modal: Free 3000 credits from GPU MODE IRL for our NVIDIA CI.</li> <li>EmbeddedLLM: Making Liger Kernel run fast and stable on AMD. </li> <li>HuggingFace: Integrating Liger Kernel into Hugging Face Transformers and TRL.</li> <li>Lightning AI: Integrating Liger Kernel into Lightning Thunder.</li> <li>Axolotl: Integrating Liger Kernel into Axolotl.</li> <li>Llama-Factory: Integrating Liger Kernel into Llama-Factory.</li> </ul> <p> Contact </p> <ul> <li>For issues, create a Github ticket in this repository .</li> <li>For open discussion, join our discord channel .</li> <li>For formal collaboration, send an email to byhsu@linkedin.com .</li> </ul>"},{"location":"#cite-this-work","title":"Cite this work","text":"<p>Bib Latex entry: <pre><code>@inproceedings{\nhsu2025ligerkernel,\ntitle={Liger-Kernel: Efficient Triton Kernels for {LLM} Training},\nauthor={Pin-Lun Hsu and Yun Dai and Vignesh Kothapalli and Qingquan Song and Shao Tang and Siyu Zhu and Steven Shimizu and Shivam Sahni and Haowen Ning and Yanning Chen and Zhipeng Wang},\nbooktitle={Championing Open-source DEvelopment in ML Workshop @ ICML25},\nyear={2025},\nurl={https://openreview.net/forum?id=36SjAIT42G}\n}\n</code></pre></p>"},{"location":"#star-history","title":"Star History","text":"<p>          \u2191 Back to Top \u2191      </p>"},{"location":"Examples/","title":"Examples","text":"<p>HANDS-ON USECASE EXAMPLES</p> Use Case Description Hugging Face Trainer Train LLaMA 3-8B ~20% faster with over 40% memory reduction on Alpaca dataset using 4 A100s with FSDP Lightning Trainer Increase 15% throughput and reduce memory usage by 40% with LLaMA3-8B on MMLU dataset using 8 A100s with DeepSpeed ZeRO3 Medusa Multi-head LLM (Retraining Phase) Reduce memory usage by 80% with 5 LM heads and improve throughput by 40% using 8 A100s with FSDP Vision-Language Model SFT Finetune Qwen2-VL on image-text data using 4 A100s with FSDP Liger ORPO Trainer Align Llama 3.2 using Liger ORPO Trainer with FSDP with 50% memory reduction"},{"location":"Examples/#huggingface-trainer","title":"HuggingFace Trainer","text":""},{"location":"Examples/#how-to-run","title":"How to Run","text":""},{"location":"Examples/#locally-on-a-gpu-machine","title":"Locally on a GPU machine","text":"<p>You can run the example locally on a GPU machine. The default hyperparameters and configurations work on single node with 4xA100 80GB GPUs and FSDP.</p> <p>Example</p> <pre><code>pip install -r requirements.txt\nsh run_{MODEL}.sh\n</code></pre>"},{"location":"Examples/#remotely-on-modal","title":"Remotely on Modal","text":"<p>If you do not have access to a GPU machine, you can run the example on Modal. Modal is a serverless platform that allows you to run your code on a remote GPU machine. You can sign up for a free account at Modal.</p> <p>Example</p> <pre><code>pip install modal\nmodal setup  # authenticate with Modal\nmodal run launch_on_modal.py --script \"run_qwen2_vl.sh\"\n</code></pre> <p>Notes</p> <ol> <li> <p>This example uses an optional <code>use_liger</code> flag. If true, it does a 1 line monkey patch to apply liger kernel.</p> </li> <li> <p>The example uses Llama3 model that requires community license agreement and HuggingFace Hub login. If you want to use Llama3 in this example, please make sure you have done the following:</p> <ul> <li>Agree on the community license agreement .</li> <li>Run <code>huggingface-cli login</code> and enter your HuggingFace token.</li> </ul> </li> <li> <p>The default hyperparameters and configurations work on single node with 4xA100 80GB GPUs. For running on device with less GPU RAM, please consider reducing the per-GPU batch size and/or enable <code>CPUOffload</code> in FSDP.</p> </li> </ol>"},{"location":"Examples/#benchmark-result","title":"Benchmark Result","text":""},{"location":"Examples/#llama","title":"Llama","text":"<p>Info</p> <p>Benchmark conditions:  Model= LLaMA 3-8B,Datset= Alpaca, Max seq len = 512, Data Type = bf16, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 4 A100s.</p> <p>Throughput improves by around 20%, while GPU memory usage drops by 40%. This allows you to train the model on smaller GPUs, use larger batch sizes, or handle longer sequence lengths without incurring additional costs.</p> <p> </p>"},{"location":"Examples/#qwen","title":"Qwen","text":"<p>Info</p> <p>Benchmark conditions: Model= Qwen2-7B, Dataset= Alpaca, Max seq len = 512, Data Type = bf16, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 4 A100s.</p> <p>Throughput improves by around 10%, while GPU memory usage drops by 50%.</p> <p> </p>"},{"location":"Examples/#gemma-7b","title":"Gemma 7B","text":"<p>Info</p> <p>Benchmark conditions: Model= Gemma-7B, Dataset= Alpaca, Max seq len = 512, Data Type = bf16, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 4 A100s.</p> <p>Throughput improves by around 24%, while GPU memory usage drops by 33%.</p> <p> </p>"},{"location":"Examples/#lightning-trainer","title":"Lightning Trainer","text":""},{"location":"Examples/#how-to-run_1","title":"How to Run","text":""},{"location":"Examples/#locally-on-a-gpu-machine_1","title":"Locally on a GPU machine","text":"<p>You can run the example locally on a GPU machine.</p> <p>Example</p> <pre><code>pip install -r requirements.txt\n\n# For single L40 48GB GPU\npython training.py --model Qwen/Qwen2-0.5B-Instruct --num_gpu 1 --max_length 1024\n\n# For 8XA100 40GB\npython training.py --model meta-llama/Meta-Llama-3-8B --strategy deepspeed\n</code></pre> <p>Notes</p> <ol> <li> <p>The example uses Llama3 model that requires community license agreement and HuggingFace Hub login. If you want to use Llama3 in this example, please make sure you have done the following:</p> <ul> <li>Agree on the community license agreement</li> <li>Run <code>huggingface-cli login</code> and enter your HuggingFace token.</li> </ul> </li> <li> <p>The default hyperparameters and configurations for gemma works on single L40 48GB GPU and config for llama work on single node with 8xA100 40GB GPUs. For running on device with less GPU RAM, please consider reducing the per-GPU batch size and/or enable <code>CPUOffload</code> in FSDP.</p> </li> </ol>"},{"location":"Examples/#medusa","title":"Medusa","text":"<p>Medusa is a simple framework that democratizes the acceleration techniques for LLM generation with multiple decoding heads. To know more, you can check out the repo and the paper .</p> <p>The Liger fused CE kernel is highly effective in this scenario, eliminating the need to materialize logits for each head, which usually consumes a large volume of memory due to the extensive vocabulary size (e.g., for LLaMA-3, the vocabulary size is 128k).</p> <p>The introduction of multiple heads can easily lead to OOM (Out of Memory) issues. However, thanks to the efficient Liger fused CE, which calculates the gradient in place and doesn't materialize the logits, we have observed very effective results. This efficiency opens up more opportunities for multi-token prediction research and development.</p>"},{"location":"Examples/#how-to-run_2","title":"How to Run","text":"<p>Example</p> <pre><code>git clone git@github.com:linkedin/Liger-Kernel.git\ncd {PATH_TO_Liger-Kernel}/Liger-Kernel/\npip install -e .\ncd {PATH_TO_Liger-Kernel}/Liger-Kernel/examples/medusa\npip install -r requirements.txt\nsh scripts/llama3_8b_medusa.sh\n</code></pre> <p>Notes</p> <ol> <li> <p>This example uses an optional <code>use_liger</code> flag. If true, it does a monkey patch to apply liger kernel with medusa heads.</p> </li> <li> <p>The example uses Llama3 model that requires community license agreement and HuggingFace Hub login. If you want to use Llama3 in this example, please make sure you have done the followings:</p> <ul> <li>Agree on the community license agreement https://huggingface.co/meta-llama/Meta-Llama-3-8B</li> <li>Run <code>huggingface-cli login</code> and enter your HuggingFace token</li> </ul> </li> <li> <p>The default hyperparameters and configurations work on single node with 8xA100 GPUs. For running on device with less GPU RAM, please consider reducing the per-GPU batch size and/or enable <code>CPUOffload</code> in FSDP.</p> </li> <li> <p>We are using a smaller sample of shared GPT data primarily to benchmark performance. The example requires hyperparameter tuning and dataset selection to work effectively, also ensuring the dataset has the same distribution as the LLaMA pretraining data. Welcome contribution to enhance the example code.</p> </li> </ol>"},{"location":"Examples/#benchmark-result_1","title":"Benchmark Result","text":"<p>Info</p> <ol> <li>Benchmark conditions: LLaMA 3-8B, Batch Size = 6, Data Type = bf16, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 8 A100s.</li> </ol>"},{"location":"Examples/#stage-1","title":"Stage 1","text":"<p>Stage 1 refers to Medusa-1 where the backbone model is frozen and only weights of LLM heads are updated.</p> <p>Warning</p> <pre><code># Modify this flag in llama3_8b_medusa.sh to True enables stage1 \n--medusa_only_heads True\n</code></pre>"},{"location":"Examples/#num_head-3","title":"num_head = 3","text":""},{"location":"Examples/#num_head-5","title":"num_head = 5","text":""},{"location":"Examples/#stage-2","title":"Stage 2","text":"<p>Warning</p> <pre><code># Modify this flag to False in llama3_8b_medusa.sh enables stage2\n--medusa_only_heads False\n</code></pre> <p>Stage 2 refers to Medusa-2 where all the model weights are updated including the backbone model and llm heads.</p>"},{"location":"Examples/#num_head-3_1","title":"num_head = 3","text":""},{"location":"Examples/#num_head-5_1","title":"num_head = 5","text":""},{"location":"Examples/#vision-language-model-sft","title":"Vision-Language Model SFT","text":""},{"location":"Examples/#how-to-run_3","title":"How to Run","text":""},{"location":"Examples/#locally-on-a-gpu-machine_2","title":"Locally on a GPU Machine","text":"<p>You can run the example locally on a GPU machine. The default hyperparameters and configurations work on single node with 4xA100 80GB GPUs.</p> <p>Example</p> <pre><code>#!/bin/bash\n\ntorchrun --nnodes=1 --nproc-per-node=4 training_multimodal.py \\\n    --model_name \"Qwen/Qwen2-VL-7B-Instruct\" \\\n    --bf16 \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 8 \\\n    --eval_strategy \"no\" \\\n    --save_strategy \"no\" \\\n    --learning_rate 6e-6 \\\n    --weight_decay 0.05 \\\n    --warmup_ratio 0.1 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --include_num_input_tokens_seen \\\n    --report_to none \\\n    --fsdp \"full_shard auto_wrap\" \\\n    --fsdp_config config/fsdp_config.json \\\n    --seed 42 \\\n    --use_liger True \\\n    --output_dir multimodal_finetuning\n</code></pre>"},{"location":"Examples/#orpo-trainer","title":"ORPO Trainer","text":""},{"location":"Examples/#how-to-run_4","title":"How to Run","text":""},{"location":"Examples/#locally-on-a-gpu-machine_3","title":"Locally on a GPU Machine","text":"<p>You can run the example locally on a GPU machine and FSDP.</p> <p>Example</p> <pre><code>import torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import ORPOConfig  # noqa: F401\n\nfrom liger_kernel.transformers.trainer import LigerORPOTrainer  # noqa: F401\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-1B-Instruct\",\n    dtype=torch.bfloat16,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"meta-llama/Llama-3.2-1B-Instruct\",\n    max_length=512,\n    padding=\"max_length\",\n)\ntokenizer.pad_token = tokenizer.eos_token\n\ntrain_dataset = load_dataset(\"trl-lib/tldr-preference\", split=\"train\")\n\ntraining_args = ORPOConfig(\n    output_dir=\"Llama3.2_1B_Instruct\",\n    beta=0.1,\n    max_length=128,\n    per_device_train_batch_size=32,\n    max_steps=100,\n    save_strategy=\"no\",\n)\n\ntrainer = LigerORPOTrainer(\n    model=model, args=training_args, tokenizer=tokenizer, train_dataset=train_dataset\n)\n\ntrainer.train()\n</code></pre>"},{"location":"Getting-Started/","title":"Getting Started","text":"<p>There are a couple of ways to apply Liger kernels, depending on the level of customization required.</p>"},{"location":"Getting-Started/#1-use-autoligerkernelforcausallm","title":"1. Use AutoLigerKernelForCausalLM","text":"<p>Using the <code>AutoLigerKernelForCausalLM</code> is the simplest approach, as you don't have to import a model-specific patching API. If the model type is supported, the modeling code will be automatically patched using the default settings.</p> <p>Example</p> <pre><code>from liger_kernel.transformers import AutoLigerKernelForCausalLM\n\n# This AutoModel wrapper class automatically monkey-patches the\n# model with the optimized Liger kernels if the model is supported.\nmodel = AutoLigerKernelForCausalLM.from_pretrained(\"path/to/some/model\")\n</code></pre>"},{"location":"Getting-Started/#2-apply-model-specific-patching-apis","title":"2. Apply Model-Specific Patching APIs","text":"<p>Using the patching APIs, you can swap Hugging Face models with optimized Liger Kernels.</p> <p>Example</p> <pre><code>import transformers\nfrom liger_kernel.transformers import apply_liger_kernel_to_llama\n\n# 1a. Adding this line automatically monkey-patches the model with the optimized Liger kernels\napply_liger_kernel_to_llama()\n\n# 1b. You could alternatively specify exactly which kernels are applied\napply_liger_kernel_to_llama(\n  rope=True,\n  swiglu=True,\n  cross_entropy=True,\n  fused_linear_cross_entropy=False,\n  rms_norm=False\n)\n\n# 2. Instantiate patched model\nmodel = transformers.AutoModelForCausalLM(\"path/to/llama/model\")\n</code></pre>"},{"location":"Getting-Started/#3-compose-your-own-model","title":"3. Compose Your Own Model","text":"<p>You can take individual kernels to compose your models.</p> <p>Example</p> <pre><code>from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss\nimport torch.nn as nn\nimport torch\n\nmodel = nn.Linear(128, 256).cuda()\n\n# fuses linear + cross entropy layers together and performs chunk-by-chunk computation to reduce memory\nloss_fn = LigerFusedLinearCrossEntropyLoss()\n\ninput = torch.randn(4, 128, requires_grad=True, device=\"cuda\")\ntarget = torch.randint(256, (4, ), device=\"cuda\")\n\nloss = loss_fn(model.weight, input, target)\nloss.backward()\n</code></pre>"},{"location":"High-Level-APIs/","title":"High-Level APIs","text":""},{"location":"High-Level-APIs/#automodel","title":"AutoModel","text":"AutoModel Variant API AutoModelForCausalLM <code>liger_kernel.transformers.AutoLigerKernelForCausalLM</code> <p>This API extends the implementation of the <code>AutoModelForCausalLM</code> within the <code>transformers</code> library from Hugging Face.</p> <p>Try it Out</p> <p>You can experiment as shown in this example here.</p>"},{"location":"High-Level-APIs/#liger_kernel.transformers.AutoLigerKernelForCausalLM","title":"liger_kernel.transformers.AutoLigerKernelForCausalLM","text":"<p>               Bases: <code>AutoModelForCausalLM</code></p> <p>This class is a drop-in replacement for AutoModelForCausalLM that applies the Liger Kernel to the model if applicable.</p> Source code in <code>src/liger_kernel/transformers/auto_model.py</code> <pre><code>class AutoLigerKernelForCausalLM(AutoModelForCausalLM):\n    \"\"\"\n    This class is a drop-in replacement for AutoModelForCausalLM that applies the Liger Kernel to the model\n    if applicable.\n    \"\"\"\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        model_config = _get_model_config(pretrained_model_name_or_path, **kwargs)\n\n        # Determine the model type and apply the Liger Kernel if applicable\n        # Note: _apply_liger_kernel will only pass relevant kwargs to the apply_liger_kernel_to_* function\n        model_type = model_config.model_type\n\n        _apply_liger_kernel(model_type, **kwargs)\n\n        # Filter out kwargs that were passed to the apply_liger_* function, which will cause\n        # model initialization errors otherwise\n        apply_fn = MODEL_TYPE_TO_APPLY_LIGER_FN[model_type]\n        apply_fn_signature = inspect.signature(apply_fn)\n\n        applicable_kwargs = {key: value for key, value in kwargs.items() if key not in apply_fn_signature.parameters}\n\n        return super().from_pretrained(pretrained_model_name_or_path, *model_args, **applicable_kwargs)\n</code></pre>"},{"location":"High-Level-APIs/#patching","title":"Patching","text":"<p>You can also use the Patching APIs to use the kernels for a specific model architecture.</p> Model API Supported Operations LLaMA 2 &amp; 3 <code>liger_kernel.transformers.apply_liger_kernel_to_llama</code> RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy LLaMA 3.2-Vision <code>liger_kernel.transformers.apply_liger_kernel_to_mllama</code> RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy Mistral <code>liger_kernel.transformers.apply_liger_kernel_to_mistral</code> RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy Mixtral <code>liger_kernel.transformers.apply_liger_kernel_to_mixtral</code> RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy Gemma1 <code>liger_kernel.transformers.apply_liger_kernel_to_gemma</code> RoPE, RMSNorm, GeGLU, CrossEntropyLoss, FusedLinearCrossEntropy Gemma2 <code>liger_kernel.transformers.apply_liger_kernel_to_gemma2</code> RoPE, RMSNorm, GeGLU, CrossEntropyLoss, FusedLinearCrossEntropy Qwen2, Qwen2.5, &amp; QwQ <code>liger_kernel.transformers.apply_liger_kernel_to_qwen2</code> RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy Qwen2-VL <code>liger_kernel.transformers.apply_liger_kernel_to_qwen2_vl</code> RMSNorm, LayerNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy Phi3 &amp; Phi3.5 <code>liger_kernel.transformers.apply_liger_kernel_to_phi3</code> RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy"},{"location":"High-Level-APIs/#function-signatures","title":"Function Signatures","text":""},{"location":"High-Level-APIs/#liger_kernel.transformers.apply_liger_kernel_to_llama","title":"liger_kernel.transformers.apply_liger_kernel_to_llama","text":"<pre><code>apply_liger_kernel_to_llama(rope=True, cross_entropy=False, fused_linear_cross_entropy=True, rms_norm=True, swiglu=True, model=None)\n</code></pre> <p>Apply Liger kernels to replace original implementation in HuggingFace Llama models (2 and 3)</p> <p>Parameters:</p> Name Type Description Default <code>rope</code> <code>bool</code> <p>Whether to apply Liger's rotary position embedding. Default is True.</p> <code>True</code> <code>cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's cross entropy loss. Default is False.</p> <code>False</code> <code>fused_linear_cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's fused linear cross entropy loss. Default is True. <code>cross_entropy</code> and <code>fused_linear_cross_entropy</code> cannot both be True. If <code>fused_linear_cross_entropy</code> is True, the logits will not be materialized but more memory efficient.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>swiglu</code> <code>bool</code> <p>Whether to apply Liger's SwiGLU MLP. Default is True.</p> <code>True</code> <code>model</code> <code>PreTrainedModel</code> <p>The model instance to apply Liger kernels to, if the model has already been</p> <code>None</code> Source code in <code>src/liger_kernel/transformers/monkey_patch.py</code> <pre><code>def apply_liger_kernel_to_llama(\n    rope: bool = True,\n    cross_entropy: bool = False,\n    fused_linear_cross_entropy: bool = True,\n    rms_norm: bool = True,\n    swiglu: bool = True,\n    model: PreTrainedModel = None,\n) -&gt; None:\n    \"\"\"\n    Apply Liger kernels to replace original implementation in HuggingFace Llama models (2 and 3)\n\n    Args:\n        rope (bool): Whether to apply Liger's rotary position embedding. Default is True.\n        cross_entropy (bool): Whether to apply Liger's cross entropy loss. Default is False.\n        fused_linear_cross_entropy (bool):\n            Whether to apply Liger's fused linear cross entropy loss. Default is True.\n            `cross_entropy` and `fused_linear_cross_entropy` cannot both be True.\n            If `fused_linear_cross_entropy` is True, the logits will not be materialized but more memory efficient.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        swiglu (bool): Whether to apply Liger's SwiGLU MLP. Default is True.\n        model (PreTrainedModel): The model instance to apply Liger kernels to, if the model has already been\n        loaded. Default is None.\n    \"\"\"\n\n    assert not (cross_entropy and fused_linear_cross_entropy), (\n        \"cross_entropy and fused_linear_cross_entropy cannot both be True.\"\n    )\n\n    from transformers.models.llama import modeling_llama\n    from transformers.models.llama.modeling_llama import LlamaModel\n\n    if rope:\n        modeling_llama.apply_rotary_pos_emb = liger_rotary_pos_emb\n    if rms_norm:\n        modeling_llama.LlamaRMSNorm = LigerRMSNorm\n    if swiglu:\n        modeling_llama.LlamaMLP = LigerSwiGLUMLP\n\n    if cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            from transformers.loss.loss_utils import nn\n\n            nn.functional.cross_entropy = liger_cross_entropy\n        else:\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            modeling_llama.CrossEntropyLoss = LigerCrossEntropyLoss\n\n    if fused_linear_cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            if model is not None:\n                model.forward = MethodType(llama_lce_forward, model)\n            else:\n                modeling_llama.LlamaForCausalLM.forward = llama_lce_forward\n        else:  # if version &lt; 4.46.1\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            if model is not None:\n                model.forward = MethodType(llama_lce_forward_deprecated, model)\n            else:\n                modeling_llama.LlamaForCausalLM.forward = llama_lce_forward_deprecated\n\n    if model is not None:\n        # The model instance already exists, so we need to additionally patch the\n        # instance variables that reference already-instantiated modules (e.g. LlamaRMSNorm or LlamaMLP)\n\n        # get the base model from the model instance\n        base_model: LlamaModel = getattr(model, model.base_model_prefix, model)\n\n        if rms_norm:\n            _patch_rms_norm_module(base_model.norm)\n\n        for decoder_layer in base_model.layers:\n            if swiglu:\n                _patch_swiglu_module(decoder_layer.mlp, LigerSwiGLUMLP)\n            if rms_norm:\n                _patch_rms_norm_module(decoder_layer.input_layernorm)\n                _patch_rms_norm_module(decoder_layer.post_attention_layernorm)\n</code></pre>"},{"location":"High-Level-APIs/#liger_kernel.transformers.apply_liger_kernel_to_mllama","title":"liger_kernel.transformers.apply_liger_kernel_to_mllama","text":"<pre><code>apply_liger_kernel_to_mllama(rope=True, cross_entropy=False, fused_linear_cross_entropy=True, layer_norm=True, rms_norm=True, swiglu=True, model=None)\n</code></pre> <p>Apply Liger kernels to replace original implementation in HuggingFace MLlama models. NOTE: MLlama is not available in transformers&lt;4.45.0</p> <p>Parameters:</p> Name Type Description Default <code>rope</code> <code>bool</code> <p>Whether to apply Liger's rotary position embedding. Default is True.</p> <code>True</code> <code>cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's cross entropy loss. Default is False.</p> <code>False</code> <code>fused_linear_cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's fused linear cross entropy loss. Default is True. <code>cross_entropy</code> and <code>fused_linear_cross_entropy</code> cannot both be True. If <code>fused_linear_cross_entropy</code> is True, the logits will not be materialized but more memory efficient.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>swiglu</code> <code>bool</code> <p>Whether to apply Liger's SwiGLU MLP. Default is True.</p> <code>True</code> <code>model</code> <code>PreTrainedModel</code> <p>The model instance to apply Liger kernels to, if the model has already been</p> <code>None</code> Source code in <code>src/liger_kernel/transformers/monkey_patch.py</code> <pre><code>def apply_liger_kernel_to_mllama(\n    rope: bool = True,\n    cross_entropy: bool = False,\n    fused_linear_cross_entropy: bool = True,\n    layer_norm: bool = True,\n    rms_norm: bool = True,\n    swiglu: bool = True,\n    model: PreTrainedModel = None,\n) -&gt; None:\n    \"\"\"\n    Apply Liger kernels to replace original implementation in HuggingFace MLlama models.\n    NOTE: MLlama is not available in transformers&lt;4.45.0\n\n    Args:\n        rope (bool): Whether to apply Liger's rotary position embedding. Default is True.\n        cross_entropy (bool): Whether to apply Liger's cross entropy loss. Default is False.\n        fused_linear_cross_entropy (bool):\n            Whether to apply Liger's fused linear cross entropy loss. Default is True.\n            `cross_entropy` and `fused_linear_cross_entropy` cannot both be True.\n            If `fused_linear_cross_entropy` is True, the logits will not be materialized but more memory efficient.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        swiglu (bool): Whether to apply Liger's SwiGLU MLP. Default is True.\n        model (PreTrainedModel): The model instance to apply Liger kernels to, if the model has already been\n        loaded. Default is None.\n    \"\"\"\n\n    assert not (cross_entropy and fused_linear_cross_entropy), (\n        \"cross_entropy and fused_linear_cross_entropy cannot both be True.\"\n    )\n\n    from transformers.models.mllama import modeling_mllama\n    from transformers.models.mllama.modeling_mllama import MllamaForCausalLM\n    from transformers.models.mllama.modeling_mllama import MllamaForConditionalGeneration\n    from transformers.models.mllama.modeling_mllama import MllamaTextModel\n    from transformers.models.mllama.modeling_mllama import MllamaVisionModel\n\n    from liger_kernel.transformers.model.mllama import lce_forward as mllama_lce_forward\n    from liger_kernel.transformers.model.mllama import lce_forward_deprecated as mllama_lce_forward_deprecated\n\n    if rope:\n        modeling_mllama.apply_rotary_pos_emb = liger_rotary_pos_emb\n    if layer_norm and model is None:\n        modeling_mllama.nn.LayerNorm = LigerLayerNorm\n    if rms_norm:\n        modeling_mllama.MllamaTextRMSNorm = LigerRMSNorm\n    if swiglu:\n        modeling_mllama.MllamaTextMLP = LigerSwiGLUMLP\n    if cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            from transformers.loss.loss_utils import nn\n\n            nn.functional.cross_entropy = liger_cross_entropy\n        else:\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            modeling_mllama.CrossEntropyLoss = LigerCrossEntropyLoss\n    if fused_linear_cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            if model is not None:\n                model.forward = MethodType(mllama_lce_forward, model)\n            else:\n                modeling_mllama.MllamaForCausalLM.forward = mllama_lce_forward\n        else:  # if version &lt; 4.46.1\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            if model is not None:\n                model.forward = MethodType(mllama_lce_forward_deprecated, model)\n            else:\n                modeling_mllama.MllamaForCausalLM.forward = mllama_lce_forward_deprecated\n\n    if model is not None:\n        # The model instance already exists, so we need to additionally patch the\n        # instance variables that reference already-instantiated modules\n\n        if isinstance(model, MllamaForConditionalGeneration):\n            language_model: MllamaForCausalLM = model.language_model\n            vision_model: MllamaVisionModel = model.vision_model\n            if isinstance(language_model, MllamaForCausalLM):\n                text_model: MllamaTextModel = language_model.model\n            else:\n                text_model = language_model\n        elif isinstance(model, MllamaForCausalLM):\n            text_model = model.model\n            vision_model = None\n        elif isinstance(model, MllamaTextModel):\n            text_model = model\n            vision_model = None\n\n        else:\n            raise ValueError(f\"Unsupported Mllama model type: {type(model)}\")\n\n        if text_model:\n            if rms_norm:\n                _patch_rms_norm_module(text_model.norm)\n            for decoder_layer in text_model.layers:\n                if swiglu:\n                    _patch_swiglu_module(decoder_layer.mlp, LigerSwiGLUMLP)\n                if rms_norm:\n                    _patch_rms_norm_module(decoder_layer.input_layernorm)\n                    _patch_rms_norm_module(decoder_layer.post_attention_layernorm)\n\n        if vision_model:\n            _patch_layer_norm_module(vision_model.layernorm_pre)\n            _patch_layer_norm_module(vision_model.layernorm_post)\n\n            for layer in vision_model.transformer.layers:\n                if layer_norm:\n                    _patch_layer_norm_module(layer.input_layernorm)\n                    _patch_layer_norm_module(layer.post_attention_layernorm)\n\n            for layer in vision_model.global_transformer.layers:\n                if layer_norm:\n                    _patch_layer_norm_module(layer.input_layernorm)\n                    _patch_layer_norm_module(layer.post_attention_layernorm)\n</code></pre>"},{"location":"High-Level-APIs/#liger_kernel.transformers.apply_liger_kernel_to_mistral","title":"liger_kernel.transformers.apply_liger_kernel_to_mistral","text":"<pre><code>apply_liger_kernel_to_mistral(rope=True, cross_entropy=False, fused_linear_cross_entropy=True, rms_norm=True, swiglu=True, model=None)\n</code></pre> <p>Apply Liger kernels to replace original implementation in HuggingFace Mistral models</p> <p>Parameters:</p> Name Type Description Default <code>rope</code> <code>bool</code> <p>Whether to apply Liger's rotary position embedding. Default is False.</p> <code>True</code> <code>cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's cross entropy loss. Default is True.</p> <code>False</code> <code>fused_linear_cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's fused linear cross entropy loss. Default is True. <code>cross_entropy</code> and <code>fused_linear_cross_entropy</code> cannot both be True. If <code>fused_linear_cross_entropy</code> is True, the logits will not be materialized but more memory efficient.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>swiglu</code> <code>bool</code> <p>Whether to apply Liger's SwiGLU MLP. Default is True.</p> <code>True</code> <code>model</code> <code>PreTrainedModel</code> <p>The model instance to apply Liger kernels to, if the model has already been</p> <code>None</code> Source code in <code>src/liger_kernel/transformers/monkey_patch.py</code> <pre><code>def apply_liger_kernel_to_mistral(\n    rope: bool = True,\n    cross_entropy: bool = False,\n    fused_linear_cross_entropy: bool = True,\n    rms_norm: bool = True,\n    swiglu: bool = True,\n    model: PreTrainedModel = None,\n) -&gt; None:\n    \"\"\"\n    Apply Liger kernels to replace original implementation in HuggingFace Mistral models\n\n    Args:\n        rope (bool): Whether to apply Liger's rotary position embedding. Default is False.\n        cross_entropy (bool): Whether to apply Liger's cross entropy loss. Default is True.\n        fused_linear_cross_entropy (bool):\n            Whether to apply Liger's fused linear cross entropy loss. Default is True.\n            `cross_entropy` and `fused_linear_cross_entropy` cannot both be True.\n            If `fused_linear_cross_entropy` is True, the logits will not be materialized but more memory efficient.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        swiglu (bool): Whether to apply Liger's SwiGLU MLP. Default is True.\n        model (PreTrainedModel): The model instance to apply Liger kernels to, if the model has already been\n        loaded. Default is None.\n    \"\"\"\n    assert not (cross_entropy and fused_linear_cross_entropy), (\n        \"cross_entropy and fused_linear_cross_entropy cannot both be True.\"\n    )\n\n    from transformers.models.mistral import modeling_mistral\n    from transformers.models.mistral.modeling_mistral import MistralModel\n\n    if rope:\n        modeling_mistral.apply_rotary_pos_emb = liger_rotary_pos_emb\n    if rms_norm:\n        modeling_mistral.MistralRMSNorm = LigerRMSNorm\n    if cross_entropy:\n        modeling_mistral.CrossEntropyLoss = LigerCrossEntropyLoss\n    if fused_linear_cross_entropy:\n        if transformer_version &gt;= version.parse(\"4.49.0\"):\n            if model is not None:\n                model.forward = MethodType(mistral_lce_forward, model)\n            else:\n                modeling_mistral.MistralForCausalLM.forward = mistral_lce_forward\n        else:\n            logger.warning(\n                \"The latest version of Liger does not support transformers &lt; 4.49.0 for llava. Please downgrade your liger version or upgrade your transformer version.\"\n            )\n            logger.warning(\"LigerFusedLinearCrossEntropy patch is not applied.\")\n\n    if swiglu:\n        modeling_mistral.MistralMLP = LigerSwiGLUMLP\n\n    if model is not None:\n        # The model instance already exists, so we need to additionally patch the\n        # instance variables that reference already-instantiated modules\n\n        # get the base model from the model instance\n        base_model: MistralModel = getattr(model, model.base_model_prefix, model)\n\n        if rms_norm:\n            _patch_rms_norm_module(base_model.norm)\n\n        for decoder_layer in base_model.layers:\n            if swiglu:\n                _patch_swiglu_module(decoder_layer.mlp, LigerSwiGLUMLP)\n            if rms_norm:\n                _patch_rms_norm_module(decoder_layer.input_layernorm)\n                _patch_rms_norm_module(decoder_layer.post_attention_layernorm)\n</code></pre>"},{"location":"High-Level-APIs/#liger_kernel.transformers.apply_liger_kernel_to_mixtral","title":"liger_kernel.transformers.apply_liger_kernel_to_mixtral","text":"<pre><code>apply_liger_kernel_to_mixtral(rope=True, cross_entropy=False, fused_linear_cross_entropy=True, rms_norm=True, swiglu=True, model=None)\n</code></pre> <p>Apply Liger kernels to replace original implementation in HuggingFace Mixtral models</p> <p>Parameters:</p> Name Type Description Default <code>rope</code> <code>bool</code> <p>Whether to apply Liger's rotary position embedding. Default is True.</p> <code>True</code> <code>cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's cross entropy loss. Default is False.</p> <code>False</code> <code>fused_linear_cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's fused linear cross entropy loss. Default is True. <code>cross_entropy</code> and <code>fused_linear_cross_entropy</code> cannot both be True. If <code>fused_linear_cross_entropy</code> is True, the logits will not be materialized but more memory efficient.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>swiglu</code> <code>bool</code> <p>Whether to apply Liger's SwiGLU MLP. Default is True.</p> <code>True</code> <code>model</code> <code>PreTrainedModel</code> <p>The model instance to apply Liger kernels to, if the model has already been</p> <code>None</code> Source code in <code>src/liger_kernel/transformers/monkey_patch.py</code> <pre><code>def apply_liger_kernel_to_mixtral(\n    rope: bool = True,\n    cross_entropy: bool = False,\n    fused_linear_cross_entropy: bool = True,\n    rms_norm: bool = True,\n    swiglu: bool = True,\n    model: PreTrainedModel = None,\n) -&gt; None:\n    \"\"\"\n    Apply Liger kernels to replace original implementation in HuggingFace Mixtral models\n\n    Args:\n        rope (bool): Whether to apply Liger's rotary position embedding. Default is True.\n        cross_entropy (bool): Whether to apply Liger's cross entropy loss. Default is False.\n        fused_linear_cross_entropy (bool):\n            Whether to apply Liger's fused linear cross entropy loss. Default is True.\n            `cross_entropy` and `fused_linear_cross_entropy` cannot both be True.\n            If `fused_linear_cross_entropy` is True, the logits will not be materialized but more memory efficient.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        swiglu (bool): Whether to apply Liger's SwiGLU MLP. Default is True.\n        model (PreTrainedModel): The model instance to apply Liger kernels to, if the model has already been\n        loaded. Default is None.\n    \"\"\"\n\n    assert not (cross_entropy and fused_linear_cross_entropy), (\n        \"cross_entropy and fused_linear_cross_entropy cannot both be True.\"\n    )\n\n    from transformers.models.mixtral import modeling_mixtral\n    from transformers.models.mixtral.modeling_mixtral import MixtralModel\n\n    if rope:\n        modeling_mixtral.apply_rotary_pos_emb = liger_rotary_pos_emb\n    if rms_norm:\n        modeling_mixtral.MixtralRMSNorm = LigerRMSNorm\n    if cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            from transformers.loss.loss_utils import nn\n\n            nn.functional.cross_entropy = liger_cross_entropy\n        else:\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            modeling_mixtral.CrossEntropyLoss = LigerCrossEntropyLoss\n\n    if fused_linear_cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            if model is not None:\n                model.forward = MethodType(mixtral_lce_forward, model)\n            else:\n                modeling_mixtral.MixtralForCausalLM.forward = mixtral_lce_forward\n        else:  # if version &lt; 4.46.1\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            if model is not None:\n                model.forward = MethodType(mixtral_lce_forward_deprecated, model)\n            else:\n                modeling_mixtral.MixtralForCausalLM.forward = mixtral_lce_forward_deprecated\n    if swiglu:\n        modeling_mixtral.MixtralBlockSparseTop2MLP = LigerBlockSparseTop2MLP\n\n    if model is not None:\n        # The model instance already exists, so we need to additionally patch the\n        # instance variables that reference already-instantiated modules\n\n        # get the base model from the model instance\n        base_model: MixtralModel = getattr(model, model.base_model_prefix, model)\n\n        if rms_norm:\n            _patch_rms_norm_module(base_model.norm)\n\n        for decoder_layer in base_model.layers:\n            if swiglu:\n                for expert in decoder_layer.block_sparse_moe.experts:\n                    _patch_swiglu_module(expert, LigerBlockSparseTop2MLP)\n            if rms_norm:\n                _patch_rms_norm_module(decoder_layer.input_layernorm)\n                _patch_rms_norm_module(decoder_layer.post_attention_layernorm)\n</code></pre>"},{"location":"High-Level-APIs/#liger_kernel.transformers.apply_liger_kernel_to_gemma","title":"liger_kernel.transformers.apply_liger_kernel_to_gemma","text":"<pre><code>apply_liger_kernel_to_gemma(rope=True, cross_entropy=False, fused_linear_cross_entropy=True, rms_norm=True, geglu=True, model=None)\n</code></pre> <p>Apply Liger kernels to replace original implementation in HuggingFace Gemma (Gemma 1 and 1.1 supported, for Gemma2 please use <code>apply_liger_kernel_to_gemma2</code> ) to make GPU go burrr.</p> <p>Parameters:</p> Name Type Description Default <code>rope</code> <code>bool</code> <p>Whether to apply Liger's rotary position embedding. Default is True.</p> <code>True</code> <code>cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's cross entropy loss. Default is False.</p> <code>False</code> <code>fused_linear_cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's fused linear cross entropy loss. Default is True. <code>cross_entropy</code> and <code>fused_linear_cross_entropy</code> cannot both be True. If <code>fused_linear_cross_entropy</code> is True, the logits will not be materialized but more memory efficient.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>geglu</code> <code>bool</code> <p>Whether to apply Liger's GeGLU MLP. Default is True.</p> <code>True</code> <code>model</code> <code>PreTrainedModel</code> <p>The model instance to apply Liger kernels to, if the model has already been</p> <code>None</code> Source code in <code>src/liger_kernel/transformers/monkey_patch.py</code> <pre><code>def apply_liger_kernel_to_gemma(\n    rope: bool = True,\n    cross_entropy: bool = False,\n    fused_linear_cross_entropy: bool = True,\n    rms_norm: bool = True,\n    geglu: bool = True,\n    model: PreTrainedModel = None,\n) -&gt; None:\n    \"\"\"\n    Apply Liger kernels to replace original implementation in HuggingFace Gemma\n    (Gemma 1 and 1.1 supported, for Gemma2 please use `apply_liger_kernel_to_gemma2` ) to make GPU go burrr.\n\n    Args:\n        rope (bool): Whether to apply Liger's rotary position embedding. Default is True.\n        cross_entropy (bool): Whether to apply Liger's cross entropy loss. Default is False.\n        fused_linear_cross_entropy (bool):\n            Whether to apply Liger's fused linear cross entropy loss. Default is True.\n            `cross_entropy` and `fused_linear_cross_entropy` cannot both be True.\n            If `fused_linear_cross_entropy` is True, the logits will not be materialized but more memory efficient.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        geglu (bool): Whether to apply Liger's GeGLU MLP. Default is True.\n        model (PreTrainedModel): The model instance to apply Liger kernels to, if the model has already been\n        loaded. Default is None.\n    \"\"\"\n    assert not (cross_entropy and fused_linear_cross_entropy), (\n        \"cross_entropy and fused_linear_cross_entropy cannot both be True.\"\n    )\n\n    from transformers.models.gemma import modeling_gemma\n    from transformers.models.gemma.modeling_gemma import GemmaModel\n\n    from liger_kernel.transformers.rms_norm import LigerRMSNormForGemma\n\n    _patch_rms_norm_module_for_gemma = partial(_patch_rms_norm_module, casting_mode=\"gemma\", offset=1.0)\n\n    if rope:\n        modeling_gemma.apply_rotary_pos_emb = liger_rotary_pos_emb\n    if rms_norm:\n        modeling_gemma.GemmaRMSNorm = LigerRMSNormForGemma\n    if cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            from transformers.loss.loss_utils import nn\n\n            nn.functional.cross_entropy = liger_cross_entropy\n        else:\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            modeling_gemma.CrossEntropyLoss = LigerCrossEntropyLoss\n    if geglu:\n        modeling_gemma.GemmaMLP = LigerGEGLUMLP\n    if fused_linear_cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            if model is not None:\n                model.forward = MethodType(gemma_lce_forward, model)\n            else:\n                modeling_gemma.GemmaForCausalLM.forward = gemma_lce_forward\n        else:  # if version &lt; 4.46.1\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            if model is not None:\n                model.forward = MethodType(gemma_lce_forward_deprecated, model)\n            else:\n                modeling_gemma.GemmaForCausalLM.forward = gemma_lce_forward_deprecated\n\n    if model is not None:\n        # The model instance already exists, so we need to additionally patch the\n        # instance variables that reference already-instantiated modules\n\n        # get the base model from the model instance\n        base_model: GemmaModel = getattr(model, model.base_model_prefix, model)\n\n        if rms_norm:\n            _patch_rms_norm_module_for_gemma(base_model.norm)\n\n        for decoder_layer in base_model.layers:\n            if geglu:\n                _patch_geglu_module(decoder_layer.mlp)\n            if rms_norm:\n                _patch_rms_norm_module_for_gemma(decoder_layer.input_layernorm)\n                _patch_rms_norm_module_for_gemma(decoder_layer.post_attention_layernorm)\n</code></pre>"},{"location":"High-Level-APIs/#liger_kernel.transformers.apply_liger_kernel_to_gemma2","title":"liger_kernel.transformers.apply_liger_kernel_to_gemma2","text":"<pre><code>apply_liger_kernel_to_gemma2(rope=True, cross_entropy=False, fused_linear_cross_entropy=True, rms_norm=True, geglu=True, model=None)\n</code></pre> <p>Apply Liger kernels to replace original implementation in HuggingFace Gemma2 (for Gemma1 please use <code>apply_liger_kernel_to_gemma</code>) to make GPU go burrr.</p> <p>Parameters:</p> Name Type Description Default <code>rope</code> <code>bool</code> <p>Whether to apply Liger's rotary position embedding. Default is True.</p> <code>True</code> <code>cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's cross entropy loss. Default is False.</p> <code>False</code> <code>fused_linear_cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's fused linear cross entropy loss. Default is True. <code>cross_entropy</code> and <code>fused_linear_cross_entropy</code> cannot both be True. If <code>fused_linear_cross_entropy</code> is True, the logits will not be materialized but more memory efficient.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>geglu</code> <code>bool</code> <p>Whether to apply Liger's GeGLU MLP. Default is True.</p> <code>True</code> <code>model</code> <code>PreTrainedModel</code> <p>The model instance to apply Liger kernels to, if the model has already been</p> <code>None</code> Source code in <code>src/liger_kernel/transformers/monkey_patch.py</code> <pre><code>def apply_liger_kernel_to_gemma2(\n    rope: bool = True,\n    cross_entropy: bool = False,\n    fused_linear_cross_entropy: bool = True,\n    rms_norm: bool = True,\n    geglu: bool = True,\n    model: PreTrainedModel = None,\n) -&gt; None:\n    \"\"\"\n    Apply Liger kernels to replace original implementation in HuggingFace Gemma2\n    (for Gemma1 please use `apply_liger_kernel_to_gemma`) to make GPU go burrr.\n\n    Args:\n        rope (bool): Whether to apply Liger's rotary position embedding. Default is True.\n        cross_entropy (bool): Whether to apply Liger's cross entropy loss. Default is False.\n        fused_linear_cross_entropy (bool):\n            Whether to apply Liger's fused linear cross entropy loss. Default is True.\n            `cross_entropy` and `fused_linear_cross_entropy` cannot both be True.\n            If `fused_linear_cross_entropy` is True, the logits will not be materialized but more memory efficient.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        geglu (bool): Whether to apply Liger's GeGLU MLP. Default is True.\n        model (PreTrainedModel): The model instance to apply Liger kernels to, if the model has already been\n        loaded. Default is None.\n    \"\"\"\n    assert not (cross_entropy and fused_linear_cross_entropy), (\n        \"cross_entropy and fused_linear_cross_entropy cannot both be True.\"\n    )\n\n    from transformers.models.gemma2 import modeling_gemma2\n    from transformers.models.gemma2.modeling_gemma2 import Gemma2Model\n\n    from liger_kernel.transformers.rms_norm import LigerRMSNormForGemma2\n\n    _patch_rms_norm_module_for_gemma2 = partial(\n        _patch_rms_norm_module, offset=1.0, casting_mode=\"gemma\", in_place=False\n    )\n\n    if rope:\n        modeling_gemma2.apply_rotary_pos_emb = liger_rotary_pos_emb\n    if rms_norm:\n        # https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/models/gemma/modeling_gemma.py#L109\n        modeling_gemma2.Gemma2RMSNorm = LigerRMSNormForGemma2\n    if cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            from transformers.loss.loss_utils import nn\n\n            nn.functional.cross_entropy = liger_cross_entropy\n        else:\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            modeling_gemma2.CrossEntropyLoss = LigerCrossEntropyLoss\n    if fused_linear_cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            if model is not None:\n                model.forward = MethodType(gemma2_lce_forward, model)\n            else:\n                modeling_gemma2.Gemma2ForCausalLM.forward = gemma2_lce_forward\n        else:\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            if model is not None:\n                model.forward = MethodType(gemma2_lce_forward_deprected, model)\n            else:\n                modeling_gemma2.Gemma2ForCausalLM.forward = gemma2_lce_forward_deprected\n    if geglu:\n        modeling_gemma2.Gemma2MLP = LigerGEGLUMLP\n\n    if model is not None:\n        # The model instance already exists, so we need to additionally patch the\n        # instance variables that reference already-instantiated modules\n\n        # get the base model from the model instance\n        base_model: Gemma2Model = getattr(model, model.base_model_prefix, model)\n\n        if rms_norm:\n            _patch_rms_norm_module_for_gemma2(base_model.norm)\n\n        for decoder_layer in base_model.layers:\n            if geglu:\n                _patch_geglu_module(decoder_layer.mlp)\n            if rms_norm:\n                _patch_rms_norm_module_for_gemma2(decoder_layer.input_layernorm)\n                _patch_rms_norm_module_for_gemma2(decoder_layer.post_attention_layernorm)\n                _patch_rms_norm_module_for_gemma2(decoder_layer.pre_feedforward_layernorm)\n                _patch_rms_norm_module_for_gemma2(decoder_layer.post_feedforward_layernorm)\n</code></pre>"},{"location":"High-Level-APIs/#liger_kernel.transformers.apply_liger_kernel_to_qwen2","title":"liger_kernel.transformers.apply_liger_kernel_to_qwen2","text":"<pre><code>apply_liger_kernel_to_qwen2(rope=True, cross_entropy=False, fused_linear_cross_entropy=True, rms_norm=True, swiglu=True, model=None)\n</code></pre> <p>Apply Liger kernels to replace original implementation in HuggingFace Qwen2 models</p> <p>Parameters:</p> Name Type Description Default <code>rope</code> <code>bool</code> <p>Whether to apply Liger's rotary position embedding. Default is True.</p> <code>True</code> <code>cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's cross entropy loss. Default is False.</p> <code>False</code> <code>fused_linear_cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's fused linear cross entropy loss. Default is True. <code>cross_entropy</code> and <code>fused_linear_cross_entropy</code> cannot both be True. If <code>fused_linear_cross_entropy</code> is True, the logits will not be materialized but more memory efficient.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>swiglu</code> <code>bool</code> <p>Whether to apply Liger's SwiGLU MLP. Default is True.</p> <code>True</code> <code>model</code> <code>PreTrainedModel</code> <p>The model instance to apply Liger kernels to, if the model has already been</p> <code>None</code> Source code in <code>src/liger_kernel/transformers/monkey_patch.py</code> <pre><code>def apply_liger_kernel_to_qwen2(\n    rope: bool = True,\n    cross_entropy: bool = False,\n    fused_linear_cross_entropy: bool = True,\n    rms_norm: bool = True,\n    swiglu: bool = True,\n    model: PreTrainedModel = None,\n) -&gt; None:\n    \"\"\"\n    Apply Liger kernels to replace original implementation in HuggingFace Qwen2 models\n\n    Args:\n        rope (bool): Whether to apply Liger's rotary position embedding. Default is True.\n        cross_entropy (bool): Whether to apply Liger's cross entropy loss. Default is False.\n        fused_linear_cross_entropy (bool):\n            Whether to apply Liger's fused linear cross entropy loss. Default is True.\n            `cross_entropy` and `fused_linear_cross_entropy` cannot both be True.\n            If `fused_linear_cross_entropy` is True, the logits will not be materialized but more memory efficient.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        swiglu (bool): Whether to apply Liger's SwiGLU MLP. Default is True.\n        model (PreTrainedModel): The model instance to apply Liger kernels to, if the model has already been\n        loaded. Default is None.\n    \"\"\"\n    assert not (cross_entropy and fused_linear_cross_entropy), (\n        \"cross_entropy and fused_linear_cross_entropy cannot both be True.\"\n    )\n\n    from transformers.models.qwen2 import modeling_qwen2\n    from transformers.models.qwen2.modeling_qwen2 import Qwen2Model\n\n    if rope:\n        modeling_qwen2.apply_rotary_pos_emb = liger_rotary_pos_emb\n    if rms_norm:\n        modeling_qwen2.Qwen2RMSNorm = LigerRMSNorm\n\n    if cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            from transformers.loss.loss_utils import nn\n\n            nn.functional.cross_entropy = liger_cross_entropy\n        else:\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            modeling_qwen2.CrossEntropyLoss = LigerCrossEntropyLoss\n\n    if fused_linear_cross_entropy:\n        if transformer_version &gt;= version.parse(SUPPORTED_TRANSFORMER_VERSION):\n            if model is not None:\n                model.forward = MethodType(qwen2_lce_forward, model)\n            else:\n                modeling_qwen2.Qwen2ForCausalLM.forward = qwen2_lce_forward\n        else:  # if version &lt; 4.46.1\n            logger.warning(TRANSFORMER_DEPRECATION_WARNING)\n            if model is not None:\n                model.forward = MethodType(qwen2_lce_forward_deprecated, model)\n            else:\n                modeling_qwen2.Qwen2ForCausalLM.forward = qwen2_lce_forward_deprecated\n\n    if swiglu:\n        modeling_qwen2.Qwen2MLP = LigerSwiGLUMLP\n\n    if model is not None:\n        # The model instance already exists, so we need to additionally patch the\n        # instance variables that reference already-instantiated modules\n\n        # get the base model from the model instance\n        base_model: Qwen2Model = getattr(model, model.base_model_prefix, model)\n\n        if rms_norm:\n            _patch_rms_norm_module(base_model.norm)\n\n        for decoder_layer in base_model.layers:\n            if swiglu:\n                _patch_swiglu_module(decoder_layer.mlp, LigerSwiGLUMLP)\n            if rms_norm:\n                _patch_rms_norm_module(decoder_layer.input_layernorm)\n                _patch_rms_norm_module(decoder_layer.post_attention_layernorm)\n    print(\"Applied Liger kernels to Qwen2\")\n</code></pre>"},{"location":"High-Level-APIs/#liger_kernel.transformers.apply_liger_kernel_to_qwen2_vl","title":"liger_kernel.transformers.apply_liger_kernel_to_qwen2_vl","text":"<pre><code>apply_liger_kernel_to_qwen2_vl(rope=True, cross_entropy=False, fused_linear_cross_entropy=True, rms_norm=True, layer_norm=True, swiglu=True, model=None)\n</code></pre> <p>Apply Liger kernels to replace original implementation in HuggingFace Qwen2-VL models. NOTE: Qwen2-VL is not supported in transformers&lt;4.52.4</p> <p>Parameters:</p> Name Type Description Default <code>cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's cross entropy loss. Default is False.</p> <code>False</code> <code>fused_linear_cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's fused linear cross entropy loss. Default is True. <code>cross_entropy</code> and <code>fused_linear_cross_entropy</code> cannot both be True. If <code>fused_linear_cross_entropy</code> is True, the logits will not be materialized but more memory efficient.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>layer_norm</code> <code>bool</code> <p>Whether to apply Liger's LayerNorm. Default is True.</p> <code>True</code> <code>swiglu</code> <code>bool</code> <p>Whether to apply Liger's SwiGLU MLP. Default is True.</p> <code>True</code> <code>model</code> <code>PreTrainedModel</code> <p>The model instance to apply Liger kernels to, if the model has already been</p> <code>None</code> Source code in <code>src/liger_kernel/transformers/monkey_patch.py</code> <pre><code>def apply_liger_kernel_to_qwen2_vl(\n    rope: bool = True,\n    cross_entropy: bool = False,\n    fused_linear_cross_entropy: bool = True,\n    rms_norm: bool = True,\n    layer_norm: bool = True,\n    swiglu: bool = True,\n    model: PreTrainedModel = None,\n) -&gt; None:\n    \"\"\"\n    Apply Liger kernels to replace original implementation in HuggingFace Qwen2-VL models.\n    NOTE: Qwen2-VL is not supported in transformers&lt;4.52.4\n\n    Args:\n        cross_entropy (bool): Whether to apply Liger's cross entropy loss. Default is False.\n        fused_linear_cross_entropy (bool):\n            Whether to apply Liger's fused linear cross entropy loss. Default is True.\n            `cross_entropy` and `fused_linear_cross_entropy` cannot both be True.\n            If `fused_linear_cross_entropy` is True, the logits will not be materialized but more memory efficient.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        layer_norm (bool): Whether to apply Liger's LayerNorm. Default is True.\n        swiglu (bool): Whether to apply Liger's SwiGLU MLP. Default is True.\n        model (PreTrainedModel): The model instance to apply Liger kernels to, if the model has already been\n        loaded. Default is None.\n    \"\"\"\n    if transformer_version &lt; version.parse(\"4.52.4\"):\n        logger.warning(\"Qwen2-VL support is only compatible with transformers &gt;= 4.52.4\")\n        return\n\n    assert not (cross_entropy and fused_linear_cross_entropy), (\n        \"cross_entropy and fused_linear_cross_entropy cannot both be True.\"\n    )\n\n    from transformers.models.qwen2_vl import modeling_qwen2_vl\n    from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VisionTransformerPretrainedModel\n    from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLForConditionalGeneration\n    from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLModel\n    from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLTextModel\n\n    from liger_kernel.transformers.model.qwen2_vl import lce_forward as qwen2_vl_lce_forward\n\n    if rope:\n        modeling_qwen2_vl.apply_multimodal_rotary_pos_emb = liger_multimodal_rotary_pos_emb\n    if rms_norm:\n        # https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L439\n        modeling_qwen2_vl.Qwen2RMSNorm = LigerRMSNorm\n    if layer_norm and model is None:\n        modeling_qwen2_vl.LayerNorm = LigerLayerNorm\n    if cross_entropy:\n        modeling_qwen2_vl.CrossEntropyLoss = LigerCrossEntropyLoss\n    if fused_linear_cross_entropy:\n        if model is not None:\n            model.forward = MethodType(qwen2_vl_lce_forward, model)\n        else:\n            modeling_qwen2_vl.Qwen2VLForConditionalGeneration.forward = qwen2_vl_lce_forward\n    if swiglu:\n        modeling_qwen2_vl.Qwen2MLP = LigerSwiGLUMLP\n\n    if model is not None:\n        # The model instance already exists, so we need to additionally patch the\n        # instance variables that reference already-instantiated modules\n\n        if isinstance(model, (Qwen2VLForConditionalGeneration, Qwen2VLModel)):\n            # Note: language_model and visual properties can be accessed throught conditional class for BC.\n            # Not sure if it is subject to changes in the future.\n            # Reference: https://github.com/huggingface/transformers/blob/v4.52.4/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L1698\n            text_model: Qwen2VLTextModel = model.language_model\n            vision_model: Qwen2VisionTransformerPretrainedModel = model.visual\n        elif isinstance(model, Qwen2VLTextModel):\n            text_model: Qwen2VLTextModel = model\n            vision_model = None\n        else:\n            # Note: Currently there's no support for patching vision model only. Feel free to raise an issue if needed.\n            raise TypeError(\n                f\"Unsupported Qwen2VL model type. `model` must be `Qwen2VLForConditionalGeneration`, `Qwen2VLModel` or `Qwen2VLTextModel`. Got: {type(model)}\"\n            )\n\n        # Patch Qwen2VisionTransformerPretrainedModel\n        if vision_model is not None:\n            for vision_block in vision_model.blocks:\n                if layer_norm:\n                    _patch_layer_norm_module(vision_block.norm1)\n                    _patch_layer_norm_module(vision_block.norm2)\n\n        # Patch Qwen2VisionTextModel\n        if text_model is not None:\n            if rms_norm:\n                _patch_rms_norm_module(text_model.norm)\n            for decoder_layer in text_model.layers:\n                if swiglu:\n                    _patch_swiglu_module(decoder_layer.mlp, LigerSwiGLUMLP)\n                if rms_norm:\n                    _patch_rms_norm_module(decoder_layer.input_layernorm)\n                    _patch_rms_norm_module(decoder_layer.post_attention_layernorm)\n</code></pre>"},{"location":"High-Level-APIs/#liger_kernel.transformers.apply_liger_kernel_to_phi3","title":"liger_kernel.transformers.apply_liger_kernel_to_phi3","text":"<pre><code>apply_liger_kernel_to_phi3(rope=True, cross_entropy=False, fused_linear_cross_entropy=True, rms_norm=True, swiglu=True, model=None)\n</code></pre> <p>Apply Liger kernels to replace original implementation in HuggingFace Phi3 models.</p> <p>Parameters:</p> Name Type Description Default <code>rope</code> <code>bool</code> <p>Whether to apply Liger's rotary position embedding. Default is True.</p> <code>True</code> <code>cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's cross entropy loss. Default is False.</p> <code>False</code> <code>fused_linear_cross_entropy</code> <code>bool</code> <p>Whether to apply Liger's fused linear cross entropy loss. Default is True. <code>cross_entropy</code> and <code>fused_linear_cross_entropy</code> cannot both be True. If <code>fused_linear_cross_entropy</code> is True, the logits will not be materialized but more memory efficient.</p> <code>True</code> <code>rms_norm</code> <code>bool</code> <p>Whether to apply Liger's RMSNorm. Default is True.</p> <code>True</code> <code>swiglu</code> <code>bool</code> <p>Whether to apply Liger's SwiGLU Phi3MLP. Default is True.</p> <code>True</code> <code>model</code> <code>PreTrainedModel</code> <p>The model instance to apply Liger kernels to, if the model has already been</p> <code>None</code> Source code in <code>src/liger_kernel/transformers/monkey_patch.py</code> <pre><code>def apply_liger_kernel_to_phi3(\n    rope: bool = True,\n    cross_entropy: bool = False,\n    fused_linear_cross_entropy: bool = True,\n    rms_norm: bool = True,\n    swiglu: bool = True,\n    model: PreTrainedModel = None,\n) -&gt; None:\n    \"\"\"\n    Apply Liger kernels to replace original implementation in HuggingFace Phi3 models.\n\n    Args:\n        rope (bool): Whether to apply Liger's rotary position embedding. Default is True.\n        cross_entropy (bool): Whether to apply Liger's cross entropy loss. Default is False.\n        fused_linear_cross_entropy (bool):\n            Whether to apply Liger's fused linear cross entropy loss. Default is True.\n            `cross_entropy` and `fused_linear_cross_entropy` cannot both be True.\n            If `fused_linear_cross_entropy` is True, the logits will not be materialized but more memory efficient.\n        rms_norm (bool): Whether to apply Liger's RMSNorm. Default is True.\n        swiglu (bool): Whether to apply Liger's SwiGLU Phi3MLP. Default is True.\n        model (PreTrainedModel): The model instance to apply Liger kernels to, if the model has already been\n        loaded. Default is None.\n    \"\"\"\n    assert not (cross_entropy and fused_linear_cross_entropy), (\n        \"cross_entropy and fused_linear_cross_entropy cannot both be True.\"\n    )\n\n    from transformers.models.phi3 import modeling_phi3\n    from transformers.models.phi3.modeling_phi3 import Phi3Model\n\n    if rope:\n        modeling_phi3.apply_rotary_pos_emb = liger_rotary_pos_emb  # Same as Gemma\n    if rms_norm:\n        modeling_phi3.Phi3RMSNorm = LigerRMSNorm  # Same as Llama\n    if swiglu:\n        modeling_phi3.Phi3MLP = LigerPhi3SwiGLUMLP\n    if cross_entropy:\n        from transformers.loss.loss_utils import nn\n\n        nn.functional.cross_entropy = liger_cross_entropy\n    if fused_linear_cross_entropy:\n        if model is not None:\n            model.forward = MethodType(phi3_lce_forward, model)\n        else:\n            modeling_phi3.Phi3ForCausalLM.forward = phi3_lce_forward\n\n    if model is not None:\n        # The model instance already exists, so we need to additionally patch the\n        # instance variables that reference already-instantiated modules\n\n        # get the base model from the model instance\n        base_model: Phi3Model = getattr(model, model.base_model_prefix, model)\n\n        if rms_norm:\n            _patch_rms_norm_module(base_model.norm)\n\n        for decoder_layer in base_model.layers:\n            if swiglu:\n                _patch_swiglu_module(decoder_layer.mlp, LigerPhi3SwiGLUMLP)\n            if rms_norm:\n                _patch_rms_norm_module(decoder_layer.input_layernorm)\n                _patch_rms_norm_module(decoder_layer.post_attention_layernorm)\n</code></pre>"},{"location":"Low-Level-APIs/","title":"Low Level APIs","text":""},{"location":"Low-Level-APIs/#model-kernels","title":"Model Kernels","text":"Kernel API RMSNorm <code>liger_kernel.transformers.LigerRMSNorm</code> LayerNorm <code>liger_kernel.transformers.LigerLayerNorm</code> RoPE <code>liger_kernel.transformers.liger_rotary_pos_emb</code> SwiGLU <code>liger_kernel.transformers.LigerSwiGLUMLP</code> GeGLU <code>liger_kernel.transformers.LigerGEGLUMLP</code> CrossEntropy <code>liger_kernel.transformers.LigerCrossEntropyLoss</code> Fused Linear CrossEntropy <code>liger_kernel.transformers.LigerFusedLinearCrossEntropyLoss</code> Multi Token Attention <code>liger_kernel.transformers.LigerMultiTokenAttention</code> Softmax <code>liger_kernel.transformers.LigerSoftmax</code> Sparsemax <code>liger_kernel.transformers.LigerSparsemax</code>"},{"location":"Low-Level-APIs/#rms-norm","title":"RMS Norm","text":"<p>RMS Norm simplifies the LayerNorm operation by eliminating mean subtraction, which reduces computational complexity while retaining effectiveness. </p> <p>This kernel performs normalization by scaling input vectors to have a unit root mean square (RMS) value. This method allows for a ~7x speed improvement and a ~3x reduction in memory footprint compared to implementations in PyTorch.</p> <p>Try it out</p> <p>You can experiment as shown in this example here.</p>"},{"location":"Low-Level-APIs/#rope","title":"RoPE","text":"<p>RoPE (Rotary Position Embedding) enhances the positional encoding used in transformer models.</p> <p>The implementation allows for effective handling of positional information without incurring significant computational overhead.</p> <p>Try it out</p> <p>You can experiment as shown in this example here.</p>"},{"location":"Low-Level-APIs/#swiglu","title":"SwiGLU","text":""},{"location":"Low-Level-APIs/#geglu","title":"GeGLU","text":""},{"location":"Low-Level-APIs/#crossentropy","title":"CrossEntropy","text":"<p>This kernel is optimized for calculating the loss function used in classification tasks. </p> <p>The  kernel achieves a ~3x execution speed increase and a ~5x reduction in memory usage for substantial vocabulary sizes compared to implementations in PyTorch.</p> <p>Try it out</p> <p>You can experiment as shown in this example here.</p>"},{"location":"Low-Level-APIs/#fused-linear-crossentropy","title":"Fused Linear CrossEntropy","text":"<p>This kernel combines linear transformations with cross-entropy loss calculations into a single operation.</p> <p>Try it out</p> <p>You can experiment as shown in this example here</p>"},{"location":"Low-Level-APIs/#multi-token-attention","title":"Multi Token Attention","text":"<p>The Multi Token Attention kernel implementation provides and optimized fused implementation of multi-token attention over the implemented Pytorch model baseline. This is a new attention mechanism that can operate on multiple Q and K inputs introduced by Meta Research.</p> <p>Paper: https://arxiv.org/abs/2504.00927</p>"},{"location":"Low-Level-APIs/#softmax","title":"Softmax","text":"<p>The Softmax kernel implementation provides an optimized implementation of the softmax operation, which is a fundamental component in neural networks for converting raw scores into probability distributions.</p> <p>The implementation shows notable speedups compared to the Softmax PyTorch implementation</p>"},{"location":"Low-Level-APIs/#sparsemax","title":"Sparsemax","text":"<p>Sparsemax is a sparse alternative to softmax that produces sparse probability distributions. This kernel implements an efficient version of the sparsemax operation that can be used as a drop-in replacement for softmax in attention mechanisms or classification tasks.</p> <p>The implementation achieves significant speed improvements and memory savings compared to standard PyTorch implementations, particularly for large input tensors.</p>"},{"location":"Low-Level-APIs/#alignment-kernels","title":"Alignment Kernels","text":"Kernel API Fused Linear CPO Loss <code>liger_kernel.chunked_loss.LigerFusedLinearCPOLoss</code> Fused Linear DPO Loss <code>liger_kernel.chunked_loss.LigerFusedLinearDPOLoss</code> Fused Linear ORPO Loss <code>liger_kernel.chunked_loss.LigerFusedLinearORPOLoss</code> Fused Linear SimPO Loss <code>liger_kernel.chunked_loss.LigerFusedLinearSimPOLoss</code>"},{"location":"Low-Level-APIs/#distillation-kernels","title":"Distillation Kernels","text":"Kernel API KLDivergence <code>liger_kernel.transformers.LigerKLDIVLoss</code> JSD <code>liger_kernel.transformers.LigerJSD</code> Fused Linear JSD <code>liger_kernel.transformers.LigerFusedLinearJSD</code>"},{"location":"Low-Level-APIs/#experimental-kernels","title":"Experimental Kernels","text":"Kernel API Embedding <code>liger_kernel.transformers.experimental.LigerEmbedding</code> Matmul int2xint8 <code>liger_kernel.transformers.experimental.matmul</code>"},{"location":"acknowledgement/","title":"Acknowledgment","text":""},{"location":"acknowledgement/#design","title":"Design","text":"<ul> <li>@claire_yishan for the LOGO design</li> <li>Wave Snippets for generating the animated code snippets</li> </ul>"},{"location":"acknowledgement/#code","title":"Code","text":"<p>We referenced or used the following projects:</p> # Project Description Location License 1 Unsloth <code>calculate_settings</code> to determine block size and warp; We reuse it for Norm and MLP Liger Kernel Utils Apache 2 Unsloth We modified and added dW calculation on top of Unsloth implementation Liger Kernel RMS Norm Apache 3 Triton tutorial We modified on top of triton tutorials Liger Kernel RMS Norm MIT 4 tiny shakespeare dataset We use tiny shakespeare dataset to conduct convergence test on mini model Liger Kernel Convergence N/A 5 Efficient Cross Entropy We use the idea of gradient-in-forward and chunking Liger Kernel Linear Cross Entropy MIT 6 Flash attn We take many optimization ideas from the work, such as tiling and recomputation BSD 7 AutoAWQ We reference the design of automodel Liger Kernel Auto Model MIT 8 llm.c We reference the design of end-to-end testing Liger Kernel Convergence Tests MIT <p>Many thanks to the contributors to these projects for their invaluable work that helped make Liger possible.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Liger-Kernel! This guide will help you set up your development environment, add a new kernel, run tests, and submit a pull request (PR).</p> <p>Note</p>"},{"location":"contributing/#maintainers","title":"Maintainers","text":"<p>@ByronHsu(admin) @qingquansong @yundai424 @kvignesh1420 @lancerts @JasonZhu1313 @shimizust</p>"},{"location":"contributing/#interested-in-the-ticket","title":"Interested in the ticket?","text":"<p>Leave <code>#take</code> in the comment and tag the maintainer. </p>"},{"location":"contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<p>Note</p> <ol> <li>Clone the Repository <code>sh git clone https://github.com/linkedin/Liger-Kernel.git cd Liger-Kernel</code></li> <li>Install Dependencies and Editable Package <pre><code>pip install . -e[dev]\n</code></pre>  If encounter error <code>no matches found: .[dev]</code>, please use  <pre><code>pip install -e .'[dev]'\n</code></pre></li> </ol>"},{"location":"contributing/#structure","title":"Structure","text":"<p>Info</p>"},{"location":"contributing/#source-code","title":"Source Code","text":"<ul> <li><code>ops/</code>: Core Triton operations.</li> <li><code>transformers/</code>: PyTorch <code>nn.Module</code> implementations built on Triton operations, compliant with the <code>transformers</code> API.</li> </ul>"},{"location":"contributing/#tests","title":"Tests","text":"<ul> <li><code>transformers/</code>: Correctness tests for the Triton-based layers.</li> <li><code>convergence/</code>: Patches Hugging Face models with all kernels, runs multiple iterations, and compares weights, logits, and loss layer-by-layer.</li> </ul>"},{"location":"contributing/#benchmark","title":"Benchmark","text":"<ul> <li><code>benchmark/</code>: Execution time and memory benchmarks compared to Hugging Face layers.</li> </ul>"},{"location":"contributing/#adding-support-for-a-new-model","title":"Adding support for a new model","text":"<p>To get familiar with the folder structure, please refer here.</p>"},{"location":"contributing/#1-figure-out-the-kernels-that-can-be-monkey-patched","title":"1 Figure out the kernels that can be monkey-patched","text":"<p>a) Check the <code>src/liger_kernel/ops</code> directory to find the kernels that can be monkey-patched.</p> <p>b) Kernels like Fused Linear Cross Entropy require a custom lce_forward function to allow monkey-patching. For adding kernels requiring a similar approach, ensure that you create the corresponding forward function in the <code>src/liger_kernel/transformers/model</code> directory.</p>"},{"location":"contributing/#2-monkey-patch-the-huggingface-model","title":"2 Monkey-patch the HuggingFace model","text":"<p>a) Add the monkey-patching code in the <code>src/liger_kernel/transformers/monkey_patch.py</code> file.</p> <p>b) Ensure that the monkey-patching function is added to the <code>__init__.py</code> file in the <code>src/liger_kernel/transformers/</code> directory.</p>"},{"location":"contributing/#3-add-unit-tests","title":"3 Add Unit Tests","text":"<p>a) Create unit tests and convergence tests for the monkey-patched model in the tests directory. Ensure that your tests cover all functionalities of the monkey-patched model.</p>"},{"location":"contributing/#adding-a-new-kernel","title":"Adding a New Kernel","text":"<p>To get familiar with the folder structure, please refer here.</p> <ol> <li> <p>Create Your Kernel Add your kernel implementation in <code>src/liger_kernel/</code>.</p> </li> <li> <p>Add Unit Tests Create unit tests and convergence tests for your kernel in the tests directory. Ensure that your tests cover all kernel functionalities.</p> </li> <li> <p>Add Benchmark Script Add a benchmarking script under <code>benchmark/scripts</code> using the naming convention <code>benchmark_{kernel_name}.py</code> showing the performance difference between the Liger kernel and HuggingFace.</p> </li> </ol>"},{"location":"contributing/#run-tests","title":"Run tests","text":""},{"location":"contributing/#use-makefile-to-run-full-tests","title":"Use Makefile to run full tests","text":"<ol> <li>Run <code>make test</code> to ensure correctness.</li> <li>Run <code>make checkstyle</code> to ensure code style.</li> <li>Run <code>make test-convergence</code> to ensure convergence.</li> </ol>"},{"location":"contributing/#run-pytest-on-single-file","title":"Run pytest on single file","text":"<p><code>python -m pytest test_sample.py::test_function_name</code></p>"},{"location":"contributing/#run-kernel-benchmarks","title":"Run kernel benchmarks","text":"<p>The <code>/benchmark</code> directory contains benchmarking scripts for the individual kernels, demonstrating differences in speed and memory usage between using Liger and HuggingFace module implementations.</p> <ol> <li>Run <code>make run-benchmarks</code> to run all benchmarking scripts and append data to <code>benchmark/data/all_benchmark_data.csv</code>.</li> <li>Existing entries that are the same (based on <code>kernel_name</code>, <code>kernel_provider</code>, <code>kernel_operation_mode</code>, <code>metric_name</code>, <code>x_name</code>, <code>x_value</code>, <code>extra_benchmark_config_str</code>, and <code>gpu_name</code>) will not be overwritten.</li> <li>Run <code>make run-benchmarks OVERWRITE=1</code> to overwrite any existing entries that have the same configuration.</li> <li>Run <code>python benchmark/scripts/benchmark_{kernel_name}.py</code> to run an individual benchmark.</li> <li>You can use the <code>benchmark/benchmarks_visualizer.py</code> script to generate visualizations from the CSV, these are then saved to the <code>benchmark/visualizations</code> directory (note: this directory is not tracked by git).</li> </ol>"},{"location":"contributing/#submit-pr","title":"Submit PR","text":"<p>Fork the repo, copy and paste the successful test logs in the PR and submit the PR followed by the PR template (example PR).</p> <p>Notice</p> <p>As a contributor, you represent that the code you submit is your original work or that of your employer (in which case you represent you have the right to bind your employer). By submitting code, you (and, if applicable, your employer) are licensing the submitted code to LinkedIn and the open source community subject to the BSD 2-Clause license.</p>"},{"location":"contributing/#release-maintainer-only","title":"Release (Maintainer only)","text":"<ol> <li>Bump the version in pyproject.toml to the desired version (for example, <code>0.2.0</code>)</li> <li>Submit a PR and merge</li> <li>Create a new release based on the current HEAD, tag name using <code>v&lt;version number&gt;</code> for example <code>v0.2.0</code>. Alternatively, If you want to create release based on a different commit hash, <code>git tag v0.2.0 &lt;commit hash&gt; &amp;&amp; git push origin v0.2.0</code>, and create release based on this tag</li> <li>Adding release note: Minimum requirement is to click the <code>Generate Release Notes</code> button that will automatically generates 1) changes included, 2) new contributors. It's good to add sections on top to highlight the important changes.</li> <li>New pip uploading will be triggered upon a new release. NOTE: Both pre-release and official release will trigger the workflow to build wheel and publish to pypi, so please be sure that step 1-3 are followed correctly!</li> </ol> <p>Notes on version</p> <p>Here we follow the sematic versioning. Denote the version as <code>major.minor.patch</code>, we increment:</p> <ul> <li>Major version when there is backward incompatible change.</li> <li>Minor version when there is new backward-compatible functionality.</li> <li>Patch version for bug fixes.</li> </ul>"},{"location":"license/","title":"License","text":"<p>This project is licensed under the BSD 2-CLAUSE License (see <code>LICENSE</code> for details). It also includes components from projects licensed under:</p> <ul> <li>Apache License 2.0 (see <code>LICENSE-APACHE-2.0</code> for details).</li> <li>MIT License (see <code>LICENSE-MIT-AutoAWQ</code> for details).</li> <li>MIT License (see <code>LICENSE-MIT-Efficient Cross Entropy</code> for details).</li> <li>MIT License (see <code>LICENSE-MIT-llmc</code> for details).</li> <li>MIT License (see <code>LICENSE-MIT-triton</code> for details).</li> </ul>"}]}